"""
å®æ—¶å‘¼å™œå£°æ£€æµ‹æ¨¡å—
ç”¨äºå®æ—¶é‡‡é›†éŸ³é¢‘ã€è¿›è¡Œæ¨ç†å¹¶è§¦å‘æŒ¯åŠ¨æé†’
"""

import os
import sys
# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥configæ¨¡å—
# å½“å‰æ–‡ä»¶åœ¨ src/realtime/realtime_detection.pyï¼Œéœ€è¦å¯¼å…¥ src/config.py
realtime_dir = os.path.dirname(os.path.abspath(__file__))  # src/realtime
src_dir = os.path.dirname(realtime_dir)  # src
sys.path.insert(0, src_dir)

import numpy as np
import pyaudio
import librosa
from collections import deque
from tensorflow.keras.models import load_model
from config import SR, N_MFCC, N_MELS, TIME_STEPS  # åªå¯¼å…¥å¸¸é‡ï¼Œä¸å¯¼å…¥è·¯å¾„
import threading
import time


class RealtimeSnoreDetector:
    """å®æ—¶å‘¼å™œå£°æ£€æµ‹å™¨ç±»"""
    
    def __init__(self, model_path, chunk_duration=1.0, overlap=0.5, 
                 vibration_callback=None, threshold=0.5):
        """
        åˆå§‹åŒ–å®æ—¶æ£€æµ‹å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            chunk_duration: æ¯æ¬¡å¤„ç†çš„éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
            overlap: çª—å£é‡å æ¯”ä¾‹ï¼ˆ0-1ï¼‰
            vibration_callback: æ£€æµ‹åˆ°å‘¼å™œå£°æ—¶çš„å›è°ƒå‡½æ•°ï¼ˆç”¨äºè§¦å‘æŒ¯åŠ¨ï¼‰
            threshold: é¢„æµ‹é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºæ˜¯å‘¼å™œå£°
        """
        self.model = load_model(model_path)
        self.chunk_duration = chunk_duration
        self.overlap = overlap
        self.vibration_callback = vibration_callback
        self.threshold = threshold
        
        # éŸ³é¢‘å‚æ•°
        self.chunk_size = int(SR * chunk_duration)
        self.hop_size = int(SR * chunk_duration * (1 - overlap))
        self.format = pyaudio.paFloat32
        self.channels = 1
        
        # éŸ³é¢‘ç¼“å†²åŒº
        self.audio_buffer = deque(maxlen=int(SR * 3))  # ä¿å­˜3ç§’éŸ³é¢‘
        
        # çŠ¶æ€æ§åˆ¶
        self.is_running = False
        self.audio_stream = None
        self.pyaudio_instance = None
        
        # è¿ç»­æ£€æµ‹è®¡æ•°ï¼ˆé¿å…è¯¯è§¦å‘ï¼‰
        self.snore_count = 0
        self.snore_threshold_count = 3  # è¿ç»­3æ¬¡æ£€æµ‹åˆ°æ‰è§¦å‘
        
    def extract_features(self, audio_data):
        """
        ä»éŸ³é¢‘æ•°æ®ä¸­æå–ç‰¹å¾
        
        Args:
            audio_data: éŸ³é¢‘æ•°ç»„
            
        Returns:
            é¢„å¤„ç†åçš„ç‰¹å¾æ•°ç»„
        """
        # æå–MFCCç‰¹å¾
        mfcc_features = librosa.feature.mfcc(y=audio_data, sr=SR, n_mfcc=N_MFCC)
        
        # æå–Melé¢‘è°±ç‰¹å¾
        mel_features = librosa.feature.melspectrogram(y=audio_data, sr=SR, n_mels=N_MELS)
        
        # åˆå¹¶ç‰¹å¾
        combined_features = np.concatenate((mfcc_features, mel_features), axis=0)
        
        # å½’ä¸€åŒ–
        mean = np.mean(combined_features)
        std = np.std(combined_features)
        if std > 0:
            normalized_features = (combined_features - mean) / std
        else:
            normalized_features = combined_features
        
        # è°ƒæ•´æ—¶é—´æ­¥é•¿åº¦
        if normalized_features.shape[1] != TIME_STEPS:
            if normalized_features.shape[1] < TIME_STEPS:
                pad_width = TIME_STEPS - normalized_features.shape[1]
                normalized_features = np.pad(
                    normalized_features, 
                    ((0, 0), (0, pad_width)), 
                    mode='constant'
                )
            else:
                normalized_features = normalized_features[:, :TIME_STEPS]
        
        return np.expand_dims(normalized_features, axis=0)
    
    def predict(self, audio_data):
        """
        å¯¹éŸ³é¢‘æ•°æ®è¿›è¡Œé¢„æµ‹
        
        Args:
            audio_data: éŸ³é¢‘æ•°ç»„
            
        Returns:
            é¢„æµ‹æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´ï¼Œæ¥è¿‘0è¡¨ç¤ºæ˜¯å‘¼å™œå£°ï¼Œæ¥è¿‘1è¡¨ç¤ºéå‘¼å™œå£°ï¼‰
            æ³¨æ„ï¼šæ¨¡å‹è®­ç»ƒæ—¶ label 0=å‘¼å™œå£°ï¼Œlabel 1=éå‘¼å™œå£°
        """
        features = self.extract_features(audio_data)
        prediction = self.model.predict(features, verbose=0)
        return prediction[0][0]
    
    def process_audio_chunk(self, audio_chunk):
        """
        å¤„ç†éŸ³é¢‘å—
        
        Args:
            audio_chunk: éŸ³é¢‘æ•°æ®å—
        """
        if len(audio_chunk) < self.chunk_size:
            # å¦‚æœéŸ³é¢‘å¤ªçŸ­ï¼Œè¿›è¡Œå¡«å……
            audio_chunk = np.pad(
                audio_chunk, 
                (0, self.chunk_size - len(audio_chunk)), 
                mode='constant'
            )
        
        # è¿›è¡Œé¢„æµ‹
        prediction = self.predict(audio_chunk)
        # æ³¨æ„ï¼šæ¨¡å‹è®­ç»ƒæ—¶ label 0=å‘¼å™œå£°ï¼Œlabel 1=éå‘¼å™œå£°
        # æ¨¡å‹è¾“å‡ºï¼šæ¥è¿‘0 = å‘¼å™œå£°ï¼Œæ¥è¿‘1 = éå‘¼å™œå£°
        # æ‰€ä»¥åˆ¤æ–­å‘¼å™œå£°çš„é€»è¾‘åº”è¯¥æ˜¯ï¼šé¢„æµ‹å€¼ < (1 - threshold)
        # ä¾‹å¦‚ï¼šthreshold=0.5ï¼Œåˆ™ prediction < 0.5 æ—¶è®¤ä¸ºæ˜¯å‘¼å™œå£°
        is_snore = prediction < (1 - self.threshold)
        
        # æ›´æ–°è®¡æ•°
        if is_snore:
            self.snore_count += 1
        else:
            self.snore_count = 0
        
        # è§¦å‘æŒ¯åŠ¨ï¼ˆè¿ç»­æ£€æµ‹åˆ°å¤šæ¬¡æ‰è§¦å‘ï¼Œé¿å…è¯¯æŠ¥ï¼‰
        if self.snore_count >= self.snore_threshold_count:
            if self.vibration_callback:
                self.vibration_callback()
            self.snore_count = 0  # é‡ç½®è®¡æ•°
        
        return prediction, is_snore
    
    def audio_callback(self, in_data, frame_count, time_info, status):
        """
        éŸ³é¢‘æµå›è°ƒå‡½æ•°
        
        Args:
            in_data: è¾“å…¥çš„éŸ³é¢‘æ•°æ®
            frame_count: å¸§æ•°
            time_info: æ—¶é—´ä¿¡æ¯
            status: çŠ¶æ€ä¿¡æ¯
            
        Returns:
            (None, pyaudio.paContinue) è¡¨ç¤ºç»§ç»­å½•éŸ³
        """
        if status:
            print(f"éŸ³é¢‘æµçŠ¶æ€: {status}")
        
        # å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„
        audio_data = np.frombuffer(in_data, dtype=np.float32)
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self.audio_buffer.extend(audio_data)
        
        return (None, pyaudio.paContinue)
    
    def start_detection(self):
        """å¼€å§‹å®æ—¶æ£€æµ‹"""
        if self.is_running:
            print("æ£€æµ‹å™¨å·²ç»åœ¨è¿è¡Œä¸­")
            return
        
        self.is_running = True
        self.pyaudio_instance = pyaudio.PyAudio()
        
        # åˆ—å‡ºå¯ç”¨çš„éŸ³é¢‘è®¾å¤‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print("\nå¯ç”¨éŸ³é¢‘è¾“å…¥è®¾å¤‡:")
        try:
            for i in range(self.pyaudio_instance.get_device_count()):
                info = self.pyaudio_instance.get_device_info_by_index(i)
                if info['maxInputChannels'] > 0:
                    print(f"  è®¾å¤‡ {i}: {info['name']} (é»˜è®¤: {info['defaultSampleRate']} Hz)")
        except:
            pass
        
        # æ‰“å¼€éŸ³é¢‘æµï¼ˆä½¿ç”¨é˜»å¡æ¨¡å¼ä»¥æé«˜å®æ—¶æ€§ï¼‰
        print(f"\næ­£åœ¨æ‰“å¼€éŸ³é¢‘æµ (é‡‡æ ·ç‡: {SR} Hz)...")
        try:
            self.audio_stream = self.pyaudio_instance.open(
                format=self.format,
                channels=self.channels,
                rate=SR,
                input=True,
                frames_per_buffer=self.hop_size,
                start=False  # æ‰‹åŠ¨å¯åŠ¨
            )
        except Exception as e:
            print(f"âŒ æ— æ³•æ‰“å¼€éŸ³é¢‘æµ: {e}")
            print("è¯·æ£€æŸ¥éº¦å…‹é£æ˜¯å¦å·²è¿æ¥å¹¶å…è®¸è®¿é—®")
            self.is_running = False
            return
        
        # å¯åŠ¨éŸ³é¢‘æµ
        self.audio_stream.start_stream()
        print("âœ“ éŸ³é¢‘æµå·²å¯åŠ¨\n")
        
        # å®æ—¶å¤„ç†å¾ªç¯
        print("="*60)
        print("å¼€å§‹å®æ—¶å‘¼å™œå£°ç›‘æ§...")
        print("="*60)
        print("æç¤º: æŒ‰ Ctrl+C åœæ­¢ç›‘æ§\n")
        
        last_process_time = time.time()
        frame_count = 0
        
        try:
            while self.is_running:
                # ä»éŸ³é¢‘æµè¯»å–æ•°æ®
                try:
                    audio_data_bytes = self.audio_stream.read(self.hop_size, exception_on_overflow=False)
                    audio_data = np.frombuffer(audio_data_bytes, dtype=np.float32)
                    
                    # æ·»åŠ åˆ°ç¼“å†²åŒº
                    self.audio_buffer.extend(audio_data)
                    
                    # å½“ç¼“å†²åŒºæœ‰è¶³å¤Ÿæ•°æ®æ—¶è¿›è¡Œå¤„ç†
                    if len(self.audio_buffer) >= self.chunk_size:
                        # è·å–æœ€è¿‘1ç§’çš„éŸ³é¢‘æ•°æ®
                        audio_chunk = np.array(list(self.audio_buffer)[-self.chunk_size:])
                        prediction, is_snore = self.process_audio_chunk(audio_chunk)
                        
                        # æ˜¾ç¤ºå®æ—¶çŠ¶æ€
                        frame_count += 1
                        current_time = time.time()
                        elapsed = current_time - last_process_time
                        
                        # æ„å»ºçŠ¶æ€æ˜¾ç¤º
                        # å°†é¢„æµ‹å€¼è½¬æ¢ä¸ºå‘¼å™œå£°æ¦‚ç‡ï¼ˆæ›´ç›´è§‚ï¼‰
                        # predictionæ¥è¿‘0è¡¨ç¤ºå‘¼å™œå£°ï¼Œæ¥è¿‘1è¡¨ç¤ºéå‘¼å™œå£°
                        snore_probability = 1 - prediction  # è½¬æ¢ä¸ºå‘¼å™œå£°æ¦‚ç‡
                        status_icon = "ğŸ”´" if is_snore else "ğŸŸ¢"
                        status_text = "æ£€æµ‹åˆ°å‘¼å™œå£°ï¼" if is_snore else "æ­£å¸¸"
                        snore_indicator = f"[è¿ç»­: {self.snore_count}/{self.snore_threshold_count}]" if is_snore else ""
                        
                        print(f"\r{status_icon} æ—¶é—´: {frame_count * self.chunk_duration * (1-self.overlap):.1f}s | "
                              f"å‘¼å™œå£°æ¦‚ç‡: {snore_probability:.3f} | {status_text} {snore_indicator}", 
                              end='', flush=True)
                        
                        # å¦‚æœæ£€æµ‹åˆ°å‘¼å™œå£°ä¸”è¾¾åˆ°é˜ˆå€¼ï¼Œæ˜¾ç¤ºæé†’
                        if self.snore_count == self.snore_threshold_count:
                            print(f"\nğŸ”” è§¦å‘æŒ¯åŠ¨æé†’ï¼é¢„æµ‹å€¼: {prediction:.3f}")
                        
                        last_process_time = current_time
                
                except Exception as e:
                    print(f"\nâš ï¸ éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
                    time.sleep(0.1)
                    continue
                
                # å°å»¶è¿Ÿï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                time.sleep(0.01)
                
        except KeyboardInterrupt:
            print("\n\nâš ï¸ æ£€æµ‹è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"\nâŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
        finally:
            self.stop_detection()
    
    def stop_detection(self):
        """åœæ­¢æ£€æµ‹"""
        self.is_running = False
        
        if self.audio_stream:
            self.audio_stream.stop_stream()
            self.audio_stream.close()
        
        if self.pyaudio_instance:
            self.pyaudio_instance.terminate()
        
        print("\nå®æ—¶æ£€æµ‹å·²åœæ­¢")


def vibration_alert():
    """æŒ¯åŠ¨æé†’å‡½æ•°ï¼ˆéœ€è¦æ ¹æ®å®é™…ç¡¬ä»¶å®ç°ï¼‰"""
    print("\nğŸ”” æ£€æµ‹åˆ°å‘¼å™œå£°ï¼è§¦å‘æŒ¯åŠ¨æé†’...")
    # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„ç¡¬ä»¶æ§åˆ¶ä»£ç 
    # ä¾‹å¦‚ï¼šæ§åˆ¶GPIOå¼•è„šã€å‘é€å‘½ä»¤åˆ°ä¸²å£ç­‰


if __name__ == "__main__":
    # è®¡ç®—é¡¹ç›®æ ¹ç›®å½•å’Œæ¨¡å‹è·¯å¾„
    project_root = os.path.dirname(os.path.dirname(src_dir))  # Snore_Detection
    models_dir = os.path.join(project_root, 'models')
    model_path = os.path.join(models_dir, 'final_snore_detection_model.h5')
    
    if not os.path.exists(model_path):
        print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥æ¨¡å‹è·¯å¾„")
        exit(1)
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = RealtimeSnoreDetector(
        model_path=model_path,
        chunk_duration=1.0,  # 1ç§’çª—å£
        overlap=0.5,  # 50%é‡å 
        vibration_callback=vibration_alert,
        threshold=0.5  # é¢„æµ‹é˜ˆå€¼
    )
    
    # å¼€å§‹æ£€æµ‹
    detector.start_detection()

